"""
Dataset Pipeline for IF Model Simulations
Processes npy files generated by integrate_fire_model.py and organizes them into train/valid/test sets
Similar structure to 2_dataset_pipeline.py but adapted for IF model data
"""

import json
import numpy as np
import pickle
import shutil
from pathlib import Path
import time
try:
    import tqdm
    HAS_TQDM = True
except ImportError:
    HAS_TQDM = False


class IFModelDatasetPipeline:
    """
    Dataset pipeline for IF model simulations
    Processes npy files and organizes them into train/validation/test sets
    """
    
    def __init__(self, root_folder_path, output_dir, train_dir, valid_dir, test_dir):
        """
        Initialize the IF model dataset pipeline
        
        Args:
            root_folder_path: Path to the root folder containing IF simulation npy files
            output_dir: Output directory for processed data
            train_dir: Directory for training data
            valid_dir: Directory for validation data
            test_dir: Directory for test data
        """
        self.root_folder_path = Path(root_folder_path)
        
        # Output directories
        self.output_dir = Path(output_dir)
        self.train_dir = Path(train_dir)
        self.valid_dir = Path(valid_dir)
        self.test_dir = Path(test_dir)
        
        # IF model parameters (from IntegrateFireNeuron class)
        self.dt = 0.1  # ms
        self.n_exc = 80  # number of excitatory synapses
        self.n_inh = 20  # number of inhibitory synapses
        
        # Create directories
        self._create_directories()
    
    def _create_directories(self):
        """Create all necessary directories"""
        for directory in [self.output_dir, self.train_dir, self.valid_dir, self.test_dir]:
            directory.mkdir(parents=True, exist_ok=True)
    
    def load_trial_data(self, trial_id):
        """
        Load pickle file for a single trial
        
        Args:
            trial_id: Trial identifier
            
        Returns:
            sim_dict: Dictionary containing simulation data in format similar to 2_dataset_pipeline.py
        """
        try:
            # Load pickle file containing the simulation dictionary
            pickle_path = self.root_folder_path / f'trial_{trial_id}.pkl'
            with open(pickle_path, 'rb') as f:
                sim_dict = pickle.load(f)
            
            # Convert to format compatible with 2_dataset_pipeline.py
            # Extract data from the dictionary
            voltage = sim_dict['voltage']
            ex_input_raster = sim_dict['exInputSpikeTimes']  # Shape: (n_exc, n_time_steps)
            inh_input_raster = sim_dict['inhInputSpikeTimes']  # Shape: (n_inh, n_time_steps)
            output_spikes = sim_dict['outputSpikeTimes']
            
            # Resample voltage to low resolution (1ms)
            ratio = int(1.0 / self.dt)  # 10 for dt=0.1ms
            new_length = (len(voltage) // ratio) * ratio
            soma_voltage_low = np.mean(voltage[:new_length].reshape(-1, ratio), axis=1)
            
            # Convert raster format back to dictionary format for compatibility
            # exInputSpikeTimes and inhInputSpikeTimes as dict: {synapse_idx: spike_times_array}
            ex_input_spikes = {}
            for syn_idx in range(self.n_exc):
                # Find spike times from raster
                spike_indices = np.where(ex_input_raster[syn_idx] > 0)[0]
                spike_times = spike_indices * self.dt 
                ex_input_spikes[syn_idx] = spike_times.astype(int) # convert to int
            
            inh_input_spikes = {}
            for syn_idx in range(self.n_inh):
                # Find spike times from raster
                spike_indices = np.where(inh_input_raster[syn_idx] > 0)[0]
                spike_times = spike_indices * self.dt
                inh_input_spikes[syn_idx] = spike_times.astype(int) # convert to int
            
            # Create sim_dict in format similar to 2_dataset_pipeline.py
            converted_dict = {
                'somaVoltageHighRes': voltage,
                'recordingTimeLowRes': np.arange(len(soma_voltage_low)),
                'somaVoltageLowRes': soma_voltage_low,
                'exInputSpikeTimes': ex_input_spikes,  # Dict: {synapse_idx: spike_times_array}
                'inhInputSpikeTimes': inh_input_spikes,  # Dict: {synapse_idx: spike_times_array}
                'outputSpikeTimes': output_spikes,
            }
            
            return converted_dict
            
        except Exception as e:
            print(f"Error loading trial {trial_id}: {e}")
            return None
    
    def convert_from_trial_ids(self, trial_ids):
        """
        Convert multiple IF simulations to dataset format
        
        Args:
            trial_ids: List of trial identifiers
            
        Returns:
            final_data: Dictionary with 'Params', 'Results', and 'TrialMapping'
        """
        all_sim_dicts = []
        trial_mapping = {}
        
        print(f"Processing {len(trial_ids)} IF simulations...")
        
        # Process with progress bar if available
        iterator = trial_ids
        if HAS_TQDM:
            iterator = tqdm.tqdm(trial_ids, desc="Processing simulations")
        
        for sim_index, trial_id in enumerate(iterator):
            try:
                sim_dict = self.load_trial_data(trial_id)
                if sim_dict is None:
                    continue
                
                trial_mapping[sim_index] = {
                    'trial_index': trial_id,
                    'sim_index': sim_index
                }
                
                all_sim_dicts.append(sim_dict)
                
            except Exception as e:
                print(f"Error processing trial {trial_id}: {e}")
        
        # Extract common parameters
        sim_params = {
            'dt': self.dt,
            'n_exc': self.n_exc,
            'n_inh': self.n_inh,
            'totalSimDurationInMs': 6000,  # ms
            'totalSimDurationInSec': 6,  # s
        }
        
        final_data = {
            'Params': sim_params,
            'Results': {'listOfSingleSimulationDicts': all_sim_dicts},
            'TrialMapping': trial_mapping
        }
        
        print(f"Successfully processed {len(all_sim_dicts)} simulations")
        return final_data
    
    def save_to_pickle(self, final_data, filename):
        """Save data to pickle file"""
        with open(filename, 'wb') as f:
            pickle.dump(final_data, f)
    
    def split_pickle_file(self, input_file='output.pkl', num_files=10):
        """Split large pickle file into smaller files with trial index tracking"""
        with open(input_file, 'rb') as f:
            data = pickle.load(f)

        all_trials = data['Results']['listOfSingleSimulationDicts']
        trial_mapping = data.get('TrialMapping', {})
        total_trials = len(all_trials)
        trials_per_file = total_trials // num_files

        print(f"Splitting {total_trials} trials into {num_files} files...")

        for i in range(num_files):
            start = i * trials_per_file
            end = (i + 1) * trials_per_file if i < (num_files - 1) else total_trials
            
            # Create sub-data
            sub_data = {
                'Params': data['Params'],
                'Results': {'listOfSingleSimulationDicts': all_trials[start:end]},
                'TrialMapping': {k: v for k, v in trial_mapping.items() if start <= k < end}
            }
            
            # Save file
            filename = self.output_dir / f'IF_model_Output_spikes_{i:04d}.p'
            with open(filename, 'wb') as f_out:
                pickle.dump(sub_data, f_out)

        print("Splitting completed!")
    
    def organize_dataset(self, train_ratio=0.7, valid_ratio=0.2, test_ratio=0.1):
        """Organize split files into train/validation/test sets with trial index tracking"""
        files = sorted(self.output_dir.glob('*.p'))
        
        if not files:
            print(f"No .p files found in {self.output_dir}")
            return []
        
        total_files = len(files)
        train_count = int(total_files * train_ratio)
        valid_count = int(total_files * valid_ratio)
        
        # Split files
        train_files = files[:train_count]
        valid_files = files[train_count:train_count + valid_count]
        test_files = files[train_count + valid_count:]
        
        # Copy files to corresponding directories
        self._copy_files(train_files, self.train_dir)
        self._copy_files(valid_files, self.valid_dir)
        self._copy_files(test_files, self.test_dir)
        
        # Extract test set trial indices
        test_trial_indices = self._extract_trial_indices_from_files(test_files)
        
        print(f"Dataset organized:")
        print(f"  Training set: {len(train_files)} files")
        print(f"  Validation set: {len(valid_files)} files")
        print(f"  Test set: {len(test_files)} files")
        print(f"  Test set trial indices: {sorted(test_trial_indices)}")
        
        # Save test set trial indices to file
        self._save_test_trial_indices(test_trial_indices)
        
        return test_trial_indices
    
    def _copy_files(self, file_list, target_dir):
        """Copy files to target directory"""
        if HAS_TQDM:
            file_list = tqdm.tqdm(file_list, desc=f"Copying to {target_dir.name}")
        for file_path in file_list:
            shutil.copy2(file_path, target_dir / file_path.name)
    
    def _extract_trial_indices_from_files(self, file_list):
        """Extract trial indices from file list"""
        all_trial_indices = []
        
        for file_path in file_list:
            try:
                with open(file_path, 'rb') as f:
                    data = pickle.load(f)
                
                trial_mapping = data.get('TrialMapping', {})
                file_trial_indices = [mapping['trial_index'] for mapping in trial_mapping.values()]
                all_trial_indices.extend(file_trial_indices)
                
            except Exception as e:
                print(f"Error reading file {file_path}: {e}")
        
        return all_trial_indices
    
    def _save_test_trial_indices(self, test_trial_indices):
        """Save test set trial indices to file"""
        test_indices_file = self.output_dir / 'test_trial_indices.json'
        with open(test_indices_file, 'w') as f:
            json.dump({
                'test_trial_indices': sorted(test_trial_indices),
                'total_test_trials': len(test_trial_indices)
            }, f, indent=2)
        
        print(f"Test trial indices saved to: {test_indices_file}")
    
    def load_test_trial_indices(self):
        """Load test set trial indices"""
        test_indices_file = self.output_dir / 'test_trial_indices.json'
        if test_indices_file.exists():
            with open(test_indices_file, 'r') as f:
                data = json.load(f)
            return data['test_trial_indices']
        else:
            print(f"Test trial indices file not found: {test_indices_file}")
            return []
    
    def run_full_pipeline(self, trial_ids, num_files=10, train_ratio=0.7, valid_ratio=0.2, test_ratio=0.1):
        """
        Run the complete pipeline from IF simulation npy files to organized dataset
        
        Args:
            trial_ids: List of trial identifiers
            num_files: Number of files to split the data into
            train_ratio: Ratio of files for training
            valid_ratio: Ratio of files for validation
            test_ratio: Ratio of files for testing
            
        Returns:
            test_trial_indices: List of trial indices in the test set
        """
        start_time = time.time()
        print(f"Starting IF model dataset pipeline...")
        print(f"Total simulations: {len(trial_ids)}")
        
        # Step 1: Convert simulation data to pickle format
        print("Step 1: Converting simulation data...")
        step1_start = time.time()
        final_data = self.convert_from_trial_ids(trial_ids)
        self.save_to_pickle(final_data, 'output.pkl')
        step1_time = time.time() - step1_start
        print(f"Step 1 completed: output.pkl created (took {step1_time:.2f}s)")
        
        # Step 2: Split large pickle file into smaller files
        print("Step 2: Splitting pickle file...")
        step2_start = time.time()
        self.split_pickle_file('output.pkl', num_files)
        step2_time = time.time() - step2_start
        print(f"Step 2 completed: Files split (took {step2_time:.2f}s)")
        
        # Step 3: Organize files into train/validation/test sets
        print("Step 3: Organizing dataset...")
        step3_start = time.time()
        test_trial_indices = self.organize_dataset(train_ratio, valid_ratio, test_ratio)
        step3_time = time.time() - step3_start
        print(f"Step 3 completed: Dataset organized (took {step3_time:.2f}s)")
        
        total_time = time.time() - start_time
        print("IF model pipeline completed successfully!")
        print(f"Total time: {total_time:.2f}s")
        print(f"Test set contains {len(test_trial_indices)} trials: {sorted(test_trial_indices)}")
        return test_trial_indices


# Example usage
if __name__ == "__main__":
    # Define paths
    root_folder_path = 'IF_model_simulations'  # Directory containing npy files
    output_dir = '/G/results/aim2_sjc/Data/IF_model_output_dataset'
    train_dir = '/G/results/aim2_sjc/Models_TCN/IF_model_InOut/data/IF_model_train'
    valid_dir = '/G/results/aim2_sjc/Models_TCN/IF_model_InOut/data/IF_model_valid'
    test_dir = '/G/results/aim2_sjc/Models_TCN/IF_model_InOut/data/IF_model_test'
    
    # Define trial IDs (should match the trial IDs used in integrate_fire_model.py)
    num_trials = 10000
    start_trial_id = 42
    trial_ids = list(range(start_trial_id, start_trial_id + num_trials))
    
    # Create pipeline instance
    pipeline = IFModelDatasetPipeline(
        root_folder_path=root_folder_path,
        output_dir=output_dir,
        train_dir=train_dir,
        valid_dir=valid_dir,
        test_dir=test_dir
    )
    
    # Run the complete pipeline
    pipeline.run_full_pipeline(
        trial_ids=trial_ids,
        num_files=num_trials // 1000 # 100
    )

